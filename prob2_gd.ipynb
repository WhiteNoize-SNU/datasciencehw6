{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_qq-N3MSOZc"
      },
      "source": [
        "## Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5hOTbpcQPndk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from itertools import accumulate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af8_OWckUzq2"
      },
      "source": [
        "## Gradient Descent Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CH2qj-muZt4P"
      },
      "outputs": [],
      "source": [
        "class SampleModel:\n",
        "    \"\"\" class for sample model with parameters a,b \"\"\"\n",
        "    \n",
        "    def __init__(self, a=0.5, b=0.5):\n",
        "        \"\"\"\n",
        "        Initialize coefficient and bias\n",
        "\n",
        "        Inputs:\n",
        "        - a: coef\n",
        "        - b: bias\n",
        "        \"\"\"\n",
        "        self.params = {'coef':a, 'bias':b}\n",
        "        self.losses = []\n",
        "        self.times = []\n",
        "\n",
        "    def data_generation(self, true_a, true_b):\n",
        "        \"\"\"\n",
        "        Generate dataset\n",
        "        \"\"\"\n",
        "        self.true_a = true_a\n",
        "        self.true_b = true_b\n",
        "\n",
        "        N = 100000\n",
        "\n",
        "        X = np.random.randn(N)\n",
        "        random_noise = np.random.randn(N) / 10\n",
        "\n",
        "        X_modified = X + random_noise\n",
        "\n",
        "        y = self.true_a*X_modified + self.true_b\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def data_generation_quad(self, true_a, true_b):\n",
        "        \"\"\"\n",
        "        Generate dataset\n",
        "        \"\"\"\n",
        "        self.true_a = true_a\n",
        "        self.true_b = true_b\n",
        "\n",
        "        N = 100000\n",
        "\n",
        "        X = np.random.randn(N)\n",
        "        random_noise = np.random.randn(N) / 10\n",
        "\n",
        "        X_modified = X + random_noise\n",
        "\n",
        "        y = np.power(self.true_a*X_modified + self.true_b, 2)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Implement forward pass for the model\n",
        "        \"\"\"\n",
        "        a, b = self.params['coef'], self.params['bias']\n",
        "        y = a * X + b\n",
        "        return y\n",
        "\n",
        "    def backward(self, X, y):\n",
        "        \"\"\"\n",
        "        Computes the gradients for each param in self.params\n",
        "        @param X: training input data (N,)\n",
        "        @param y: training output data (N,)\n",
        "        @return: gradients of parameters\n",
        "        \"\"\"\n",
        "        a, b = self.params['coef'], self.params['bias']\n",
        "\n",
        "        gradients = {}     ## Loss(L2) = avg((y - (a*X + b))^2)\n",
        "        gradients['coef'] = np.mean(2*(y - (a*X+b))*(-X))\n",
        "        gradients['bias']  = np.mean(2*(y - (a*X+b))*(-1))\n",
        "\n",
        "        return gradients\n",
        "\n",
        "    def run(self, X, y, train=False, n_epochs=10, lr=0.001, batch_size=None):\n",
        "        \"\"\"\n",
        "        Runs the model with training as an option.\n",
        "        @param X: training input data (N,)\n",
        "        @param y: training output data (N,)\n",
        "        @train: boolean for train\n",
        "        @n_epochs: number of traninig epochs\n",
        "        @lr: learning rate\n",
        "        \"\"\"\n",
        "        print(\"Training Starts...\")\n",
        "        if train:\n",
        "            if batch_size:\n",
        "                print(\"Mini-Batch SGD w Batch Size: {}\".format(batch_size))\n",
        "                self.batch_gradient_descent(X, y, n_epochs=n_epochs, lr=lr, batch_size=batch_size)\n",
        "            \n",
        "            else:\n",
        "                print(\"Full-Batch GD\")\n",
        "                self.gradient_descent(X, y, n_epochs=n_epochs, lr=lr)\n",
        "\n",
        "    def gradient_descent(self, X, y, n_epochs=10, lr=0.001):\n",
        "        \"\"\"\n",
        "        Train using batch gradient descent.\n",
        "        @param X: training input data (N,)\n",
        "        @param y: training output data (N,)\n",
        "        @param lr: learning rate\n",
        "        \"\"\"\n",
        "        for epoch in range(n_epochs):\n",
        "            start = time.time()\n",
        "            gradients = self.backward(X, y)\n",
        "            for param in self.params:\n",
        "                self.params[param] -= lr * gradients[param]\n",
        "\n",
        "            current_loss = self.compute_loss(X, y)\n",
        "            end = time.time()\n",
        "            print(end - start)\n",
        "            self.losses.append(current_loss)\n",
        "            self.times.append(end - start)\n",
        "\n",
        "            print(\"========== Epoch {}/{} ==========\".format(epoch+1, n_epochs))\n",
        "            print(\"Loss > {:.2f}\".format(current_loss))\n",
        "            print(\"Params > coef: {:.2f} / bias: {:.2f}\".format(self.params['coef'], self.params['bias']))\n",
        "        self.times = list(accumulate(self.times))\n",
        "\n",
        "    def batch_gradient_descent(self, X, y, n_epochs=10, lr=0.001, batch_size=16):\n",
        "        \"\"\"\n",
        "        Train using batch gradient descent.\n",
        "        @param X: training input data (N,)\n",
        "        @param y: training output data (N,)\n",
        "        @param lr: learning rate\n",
        "        @param batch_size\n",
        "        \"\"\"\n",
        "        for epoch in range(n_epochs):\n",
        "            # Prob-(a)\n",
        "            ## TODO (Start) ##\n",
        "            start = time.time()\n",
        "            # Creating mini barches\n",
        "            mini_batches = []\n",
        "            data = np.hstack((X.reshape((-1, 1)), y.reshape((-1, 1))))\n",
        "            np.random.shuffle(data)\n",
        "            num_batches = data.shape[0] // batch_size\n",
        "\n",
        "            for i in range(num_batches):\n",
        "                batch = data[i*batch_size:(i+1)*batch_size, :]\n",
        "                X_batch = batch[:, 0]\n",
        "                y_batch = batch[:, 1]\n",
        "                mini_batches.append((X_batch, y_batch))\n",
        "            # Take the rest of the data\n",
        "            if data.shape[0] % batch_size != 0:\n",
        "                batch = data[batch_size*num_batches+1:, :]\n",
        "                X_batch = batch[:, 0]\n",
        "                y_batch = batch[:, 1]\n",
        "                mini_batches.append((X_batch, y_batch))\n",
        "            for batch in mini_batches:\n",
        "                X_batch, y_batch = batch\n",
        "                gradients = self.backward(X_batch, y_batch)\n",
        "                \n",
        "                for param in self.params:\n",
        "                    self.params[param] -= lr * gradients[param]\n",
        "            current_loss = self.compute_loss(X, y)\n",
        "            end = time.time()\n",
        "            self.losses.append(current_loss)\n",
        "            self.times.append(end - start)\n",
        "            ## TODO (End) ##\n",
        "\n",
        "            print(\"========== Epoch {}/{} ==========\".format(epoch+1, n_epochs))\n",
        "            print(\"Loss > {:.2f}\".format(current_loss))\n",
        "            print(\"Params > coef: {:.2f} / bias: {:.2f}\".format(self.params['coef'], self.params['bias']))\n",
        "        self.times = list(accumulate(self.times))\n",
        "\n",
        "    def compute_loss(self, X, y):\n",
        "        \"\"\"\n",
        "        Computes the L2 loss of the model given X, y.\n",
        "        @param X: training input data (N,)\n",
        "        @param y: training output data (N,)\n",
        "        @return: loss\n",
        "        \"\"\"\n",
        "        a, b = self.params['coef'], self.params['bias']\n",
        "        loss = np.mean(np.power(y - (a*X + b), 2))\n",
        "        return loss\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBe7l0V7dIKR"
      },
      "source": [
        "## Test Full-Batch GD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cDT4WTYqOYS6"
      },
      "outputs": [],
      "source": [
        "model = SampleModel(a=0.5, b=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Egqtk40qtrQa"
      },
      "outputs": [],
      "source": [
        "X, y = model.data_generation(true_a=20, true_b=5)\n",
        "\n",
        "## Use below code instead while doing Prob-(c)\n",
        "# X, y = model.data_generation_quad(true_a=20, true_b=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7Tmsvzqu26v",
        "outputId": "20593f95-24f0-4316-ed01-73b1ea6f1573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Starts...\n",
            "Full-Batch GD\n",
            "0.006707429885864258\n",
            "========== Epoch 1/25 ==========\n",
            "Loss > 260.32\n",
            "Params > coef: 4.40 / bias: 1.41\n",
            "0.005101203918457031\n",
            "========== Epoch 2/25 ==========\n",
            "Loss > 167.96\n",
            "Params > coef: 7.52 / bias: 2.14\n",
            "0.004578590393066406\n",
            "========== Epoch 3/25 ==========\n",
            "Loss > 108.88\n",
            "Params > coef: 10.02 / bias: 2.72\n",
            "0.00426793098449707\n",
            "========== Epoch 4/25 ==========\n",
            "Loss > 71.09\n",
            "Params > coef: 12.01 / bias: 3.18\n",
            "0.004174709320068359\n",
            "========== Epoch 5/25 ==========\n",
            "Loss > 46.91\n",
            "Params > coef: 13.61 / bias: 3.55\n",
            "0.004005908966064453\n",
            "========== Epoch 6/25 ==========\n",
            "Loss > 31.45\n",
            "Params > coef: 14.89 / bias: 3.84\n",
            "0.003974199295043945\n",
            "========== Epoch 7/25 ==========\n",
            "Loss > 21.56\n",
            "Params > coef: 15.91 / bias: 4.08\n",
            "0.0038754940032958984\n",
            "========== Epoch 8/25 ==========\n",
            "Loss > 15.23\n",
            "Params > coef: 16.73 / bias: 4.27\n",
            "0.003906965255737305\n",
            "========== Epoch 9/25 ==========\n",
            "Loss > 11.18\n",
            "Params > coef: 17.38 / bias: 4.41\n",
            "0.003918647766113281\n",
            "========== Epoch 10/25 ==========\n",
            "Loss > 8.59\n",
            "Params > coef: 17.90 / bias: 4.53\n",
            "0.00426936149597168\n",
            "========== Epoch 11/25 ==========\n",
            "Loss > 6.94\n",
            "Params > coef: 18.32 / bias: 4.63\n",
            "0.004709005355834961\n",
            "========== Epoch 12/25 ==========\n",
            "Loss > 5.88\n",
            "Params > coef: 18.66 / bias: 4.70\n",
            "0.0037767887115478516\n",
            "========== Epoch 13/25 ==========\n",
            "Loss > 5.20\n",
            "Params > coef: 18.92 / bias: 4.76\n",
            "0.0031785964965820312\n",
            "========== Epoch 14/25 ==========\n",
            "Loss > 4.76\n",
            "Params > coef: 19.14 / bias: 4.81\n",
            "0.003298521041870117\n",
            "========== Epoch 15/25 ==========\n",
            "Loss > 4.49\n",
            "Params > coef: 19.31 / bias: 4.85\n",
            "0.003119945526123047\n",
            "========== Epoch 16/25 ==========\n",
            "Loss > 4.31\n",
            "Params > coef: 19.45 / bias: 4.88\n",
            "0.0031197071075439453\n",
            "========== Epoch 17/25 ==========\n",
            "Loss > 4.20\n",
            "Params > coef: 19.56 / bias: 4.90\n",
            "0.002972841262817383\n",
            "========== Epoch 18/25 ==========\n",
            "Loss > 4.12\n",
            "Params > coef: 19.65 / bias: 4.92\n",
            "0.0034246444702148438\n",
            "========== Epoch 19/25 ==========\n",
            "Loss > 4.08\n",
            "Params > coef: 19.72 / bias: 4.94\n",
            "0.003589630126953125\n",
            "========== Epoch 20/25 ==========\n",
            "Loss > 4.05\n",
            "Params > coef: 19.77 / bias: 4.95\n",
            "0.0035254955291748047\n",
            "========== Epoch 21/25 ==========\n",
            "Loss > 4.03\n",
            "Params > coef: 19.82 / bias: 4.96\n",
            "0.0035212039947509766\n",
            "========== Epoch 22/25 ==========\n",
            "Loss > 4.02\n",
            "Params > coef: 19.85 / bias: 4.97\n",
            "0.003536701202392578\n",
            "========== Epoch 23/25 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.88 / bias: 4.97\n",
            "0.0034265518188476562\n",
            "========== Epoch 24/25 ==========\n",
            "Loss > 4.00\n",
            "Params > coef: 19.90 / bias: 4.98\n",
            "0.003405332565307617\n",
            "========== Epoch 25/25 ==========\n",
            "Loss > 4.00\n",
            "Params > coef: 19.92 / bias: 4.98\n",
            "Time spent for model loss to be converged : 0.07997\n",
            "Total epoch : 19\n",
            "Training Ends...\n",
            "\n",
            "Trained with 25 epochs, 0.1 learning rate\n",
            "Time Cost: 0.10481 sec\n"
          ]
        }
      ],
      "source": [
        "n_epochs, lr = 25, 1e-1\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "model.run(X, y, train=True, n_epochs=n_epochs, lr=lr)\n",
        "loss = model.compute_loss(X, y)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Time spent to converge\n",
        "for i in range(1, n_epochs):\n",
        "    loss_difference_rate = np.abs((model.losses[i-1] - model.losses[i]) / model.losses[i-1] * 100)\n",
        "\n",
        "    if loss_difference_rate < 1:\n",
        "        print(\"Time spent for model loss to be converged : {:.5f}\".format(model.times[i]))\n",
        "        print(\"Total epoch : {}\".format(i))\n",
        "        break\n",
        "\n",
        "print(\"Training Ends...\")\n",
        "print()\n",
        "print(\"Trained with {} epochs, {} learning rate\".format(n_epochs, lr))\n",
        "print(\"Time Cost: {:.5f} sec\".format(end_time-start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pOXQulGdNQE"
      },
      "source": [
        "## Test SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EhD_524TkKOC"
      },
      "outputs": [],
      "source": [
        "model = SampleModel(a=0.5, b=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gdNchXnokKZf"
      },
      "outputs": [],
      "source": [
        "X, y = model.data_generation(true_a=20, true_b=5)\n",
        "\n",
        "## Use below code instead while doing Prob-(c)\n",
        "# X, y = model.data_generation_quad(true_a=20, true_b=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXC8rq4nkK2C",
        "outputId": "b0b24a14-3951-4550-9c21-b394a120396e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Starts...\n",
            "Mini-Batch SGD w Batch Size: 1\n",
            "========== Epoch 1/30 ==========\n",
            "Loss > 4.87\n",
            "Params > coef: 19.10 / bias: 5.25\n",
            "========== Epoch 2/30 ==========\n",
            "Loss > 5.56\n",
            "Params > coef: 18.75 / bias: 5.05\n",
            "========== Epoch 3/30 ==========\n",
            "Loss > 4.30\n",
            "Params > coef: 20.20 / bias: 5.51\n",
            "========== Epoch 4/30 ==========\n",
            "Loss > 6.19\n",
            "Params > coef: 21.26 / bias: 4.23\n",
            "========== Epoch 5/30 ==========\n",
            "Loss > 4.12\n",
            "Params > coef: 20.34 / bias: 5.10\n",
            "========== Epoch 6/30 ==========\n",
            "Loss > 4.12\n",
            "Params > coef: 20.07 / bias: 5.35\n",
            "========== Epoch 7/30 ==========\n",
            "Loss > 4.82\n",
            "Params > coef: 19.08 / bias: 4.99\n",
            "========== Epoch 8/30 ==========\n",
            "Loss > 4.05\n",
            "Params > coef: 19.76 / bias: 5.06\n",
            "========== Epoch 9/30 ==========\n",
            "Loss > 4.58\n",
            "Params > coef: 20.52 / bias: 5.55\n",
            "========== Epoch 10/30 ==========\n",
            "Loss > 6.33\n",
            "Params > coef: 20.23 / bias: 6.50\n",
            "========== Epoch 11/30 ==========\n",
            "Loss > 4.47\n",
            "Params > coef: 20.22 / bias: 4.33\n",
            "========== Epoch 12/30 ==========\n",
            "Loss > 4.45\n",
            "Params > coef: 20.40 / bias: 5.54\n",
            "========== Epoch 13/30 ==========\n",
            "Loss > 5.61\n",
            "Params > coef: 18.94 / bias: 5.70\n",
            "========== Epoch 14/30 ==========\n",
            "Loss > 4.38\n",
            "Params > coef: 19.43 / bias: 5.27\n",
            "========== Epoch 15/30 ==========\n",
            "Loss > 5.05\n",
            "Params > coef: 19.11 / bias: 5.51\n",
            "========== Epoch 16/30 ==========\n",
            "Loss > 4.72\n",
            "Params > coef: 20.60 / bias: 5.60\n",
            "========== Epoch 17/30 ==========\n",
            "Loss > 4.47\n",
            "Params > coef: 19.33 / bias: 4.79\n",
            "========== Epoch 18/30 ==========\n",
            "Loss > 4.87\n",
            "Params > coef: 20.84 / bias: 5.39\n",
            "========== Epoch 19/30 ==========\n",
            "Loss > 6.38\n",
            "Params > coef: 21.45 / bias: 5.51\n",
            "========== Epoch 20/30 ==========\n",
            "Loss > 4.37\n",
            "Params > coef: 20.36 / bias: 5.49\n",
            "========== Epoch 21/30 ==========\n",
            "Loss > 4.17\n",
            "Params > coef: 19.97 / bias: 5.42\n",
            "========== Epoch 22/30 ==========\n",
            "Loss > 4.66\n",
            "Params > coef: 19.34 / bias: 5.48\n",
            "========== Epoch 23/30 ==========\n",
            "Loss > 5.80\n",
            "Params > coef: 21.33 / bias: 5.08\n",
            "========== Epoch 24/30 ==========\n",
            "Loss > 5.30\n",
            "Params > coef: 20.39 / bias: 3.91\n",
            "========== Epoch 25/30 ==========\n",
            "Loss > 5.94\n",
            "Params > coef: 21.21 / bias: 5.66\n",
            "========== Epoch 26/30 ==========\n",
            "Loss > 6.13\n",
            "Params > coef: 19.95 / bias: 3.53\n",
            "========== Epoch 27/30 ==========\n",
            "Loss > 4.24\n",
            "Params > coef: 19.63 / bias: 5.33\n",
            "========== Epoch 28/30 ==========\n",
            "Loss > 6.28\n",
            "Params > coef: 18.76 / bias: 5.86\n",
            "========== Epoch 29/30 ==========\n",
            "Loss > 4.30\n",
            "Params > coef: 20.37 / bias: 4.58\n",
            "========== Epoch 30/30 ==========\n",
            "Loss > 4.80\n",
            "Params > coef: 19.79 / bias: 4.11\n",
            "Time spent for model loss to be converged : 18.01322\n",
            "Total epoch : 5\n",
            "Training Ends...\n",
            "\n",
            "Trained with 30 epochs, 0.1 learning rate\n",
            "Time Cost: 89.39801 sec\n"
          ]
        }
      ],
      "source": [
        "n_epochs, lr, batch_size = 30, 1e-1, 1\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "model.run(X, y, train=True, n_epochs=n_epochs, lr=lr, batch_size=batch_size)\n",
        "loss = model.compute_loss(X, y)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Time spent to converge\n",
        "for i in range(1, n_epochs):\n",
        "    loss_difference_rate = np.abs((model.losses[i-1] - model.losses[i]) / model.losses[i-1] * 100)\n",
        "\n",
        "    if loss_difference_rate < 1:\n",
        "        print(\"Time spent for model loss to be converged : {:.5f}\".format(model.times[i]))\n",
        "        print(\"Total epoch : {}\".format(i))\n",
        "        break\n",
        "\n",
        "print(\"Training Ends...\")\n",
        "print()\n",
        "print(\"Trained with {} epochs, {} learning rate\".format(n_epochs, lr))\n",
        "print(\"Time Cost: {:.5f} sec\".format(end_time-start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoaNYNFukGqx"
      },
      "source": [
        "## Test Mini-Batch SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YwbuV1afdOdj"
      },
      "outputs": [],
      "source": [
        "model = SampleModel(a=0.5, b=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HvAV_8R2dO10"
      },
      "outputs": [],
      "source": [
        "X, y = model.data_generation(true_a=20, true_b=5)\n",
        "\n",
        "## Use below code instead while doing Prob-(c)\n",
        "# X, y = model.data_generation_quad(true_a=20, true_b=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4EvBKQcxd6Sg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Starts...\n",
            "Mini-Batch SGD w Batch Size: 1000\n",
            "========== Epoch 1/15 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 20.04 / bias: 5.04\n",
            "========== Epoch 2/15 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 20.06 / bias: 5.04\n",
            "========== Epoch 3/15 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 20.00 / bias: 5.05\n",
            "========== Epoch 4/15 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 20.03 / bias: 4.98\n",
            "========== Epoch 5/15 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 20.00 / bias: 5.04\n",
            "========== Epoch 6/15 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 20.01 / bias: 5.03\n",
            "========== Epoch 7/15 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 20.01 / bias: 4.99\n",
            "========== Epoch 8/15 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 20.00 / bias: 5.04\n",
            "========== Epoch 9/15 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 20.04 / bias: 4.99\n",
            "========== Epoch 10/15 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 20.05 / bias: 5.03\n",
            "========== Epoch 11/15 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 20.00 / bias: 4.99\n",
            "========== Epoch 12/15 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.99 / bias: 5.04\n",
            "========== Epoch 13/15 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.98 / bias: 5.02\n",
            "========== Epoch 14/15 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.97 / bias: 5.00\n",
            "========== Epoch 15/15 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 20.00 / bias: 5.00\n",
            "Time spent for model loss to be converged : 0.20367\n",
            "Total epoch : 1\n",
            "Training Ends...\n",
            "\n",
            "Trained with 15 epochs, 0.1 learning rate\n",
            "Time Cost: 1.40664 sec\n"
          ]
        }
      ],
      "source": [
        "n_epochs, lr, batch_size = 15, 1e-1, 1000\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "model.run(X, y, train=True, n_epochs=n_epochs, lr=lr, batch_size=batch_size)\n",
        "loss = model.compute_loss(X, y)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Time spent to converge\n",
        "for i in range(1, n_epochs):\n",
        "    loss_difference_rate = np.abs((model.losses[i-1] - model.losses[i]) / model.losses[i-1] * 100)\n",
        "\n",
        "    if loss_difference_rate < 1:\n",
        "        print(\"Time spent for model loss to be converged : {:.5f}\".format(model.times[i]))\n",
        "        print(\"Total epoch : {}\".format(i))\n",
        "        break\n",
        "\n",
        "print(\"Training Ends...\")\n",
        "print()\n",
        "print(\"Trained with {} epochs, {} learning rate\".format(n_epochs, lr))\n",
        "print(\"Time Cost: {:.5f} sec\".format(end_time-start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVpJJ9pf_vOm"
      },
      "source": [
        "      Write your answer to (b) in this cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZKnBGar_4Bf"
      },
      "source": [
        "      Write your answer to (c) in this cell."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "prob2_gd.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
