{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_qq-N3MSOZc"
      },
      "source": [
        "## Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5hOTbpcQPndk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from itertools import accumulate\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af8_OWckUzq2"
      },
      "source": [
        "## Gradient Descent Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CH2qj-muZt4P"
      },
      "outputs": [],
      "source": [
        "class SampleModel:\n",
        "    \"\"\" class for sample model with parameters a,b \"\"\"\n",
        "    \n",
        "    def __init__(self, a=0.5, b=0.5):\n",
        "        \"\"\"\n",
        "        Initialize coefficient and bias\n",
        "\n",
        "        Inputs:\n",
        "        - a: coef\n",
        "        - b: bias\n",
        "        \"\"\"\n",
        "        self.params = {'coef':a, 'bias':b}\n",
        "        self.losses = []\n",
        "        self.times = []\n",
        "\n",
        "    def data_generation(self, true_a, true_b):\n",
        "        \"\"\"\n",
        "        Generate dataset\n",
        "        \"\"\"\n",
        "        self.true_a = true_a\n",
        "        self.true_b = true_b\n",
        "\n",
        "        N = 100000\n",
        "\n",
        "        X = np.random.randn(N)\n",
        "        random_noise = np.random.randn(N) / 10\n",
        "\n",
        "        X_modified = X + random_noise\n",
        "\n",
        "        y = self.true_a*X_modified + self.true_b\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def data_generation_quad(self, true_a, true_b):\n",
        "        \"\"\"\n",
        "        Generate dataset\n",
        "        \"\"\"\n",
        "        self.true_a = true_a\n",
        "        self.true_b = true_b\n",
        "\n",
        "        N = 100000\n",
        "\n",
        "        X = np.random.randn(N)\n",
        "        random_noise = np.random.randn(N) / 10\n",
        "\n",
        "        X_modified = X + random_noise\n",
        "\n",
        "        y = np.power(self.true_a*X_modified + self.true_b, 2)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Implement forward pass for the model\n",
        "        \"\"\"\n",
        "        a, b = self.params['coef'], self.params['bias']\n",
        "        y = a * X + b\n",
        "        return y\n",
        "\n",
        "    def backward(self, X, y):\n",
        "        \"\"\"\n",
        "        Computes the gradients for each param in self.params\n",
        "        @param X: training input data (N,)\n",
        "        @param y: training output data (N,)\n",
        "        @return: gradients of parameters\n",
        "        \"\"\"\n",
        "        a, b = self.params['coef'], self.params['bias']\n",
        "\n",
        "        gradients = {}     ## Loss(L2) = avg((y - (a*X + b))^2)\n",
        "        gradients['coef'] = np.mean(2*(y - (a*X+b))*(-X))\n",
        "        gradients['bias']  = np.mean(2*(y - (a*X+b))*(-1))\n",
        "\n",
        "        return gradients\n",
        "\n",
        "    def run(self, X, y, train=False, n_epochs=10, lr=0.001, batch_size=None):\n",
        "        \"\"\"\n",
        "        Runs the model with training as an option.\n",
        "        @param X: training input data (N,)\n",
        "        @param y: training output data (N,)\n",
        "        @train: boolean for train\n",
        "        @n_epochs: number of traninig epochs\n",
        "        @lr: learning rate\n",
        "        \"\"\"\n",
        "        print(\"Training Starts...\")\n",
        "        if train:\n",
        "            if batch_size:\n",
        "                print(\"Mini-Batch SGD w Batch Size: {}\".format(batch_size))\n",
        "                self.batch_gradient_descent(X, y, n_epochs=n_epochs, lr=lr, batch_size=batch_size)\n",
        "            \n",
        "            else:\n",
        "                print(\"Full-Batch GD\")\n",
        "                self.gradient_descent(X, y, n_epochs=n_epochs, lr=lr)\n",
        "\n",
        "    def gradient_descent(self, X, y, n_epochs=10, lr=0.001):\n",
        "        \"\"\"\n",
        "        Train using batch gradient descent.\n",
        "        @param X: training input data (N,)\n",
        "        @param y: training output data (N,)\n",
        "        @param lr: learning rate\n",
        "        \"\"\"\n",
        "        for epoch in range(n_epochs):\n",
        "            start = time.time()\n",
        "            gradients = self.backward(X, y)\n",
        "            for param in self.params:\n",
        "                self.params[param] -= lr * gradients[param]\n",
        "\n",
        "            current_loss = self.compute_loss(X, y)\n",
        "            end = time.time()\n",
        "            print(end - start)\n",
        "            self.losses.append(current_loss)\n",
        "            self.times.append(end - start)\n",
        "\n",
        "            print(\"========== Epoch {}/{} ==========\".format(epoch+1, n_epochs))\n",
        "            print(\"Loss > {:.2f}\".format(current_loss))\n",
        "            print(\"Params > coef: {:.2f} / bias: {:.2f}\".format(self.params['coef'], self.params['bias']))\n",
        "        self.times = list(accumulate(self.times))\n",
        "\n",
        "    def batch_gradient_descent(self, X, y, n_epochs=10, lr=0.001, batch_size=16):\n",
        "        \"\"\"\n",
        "        Train using batch gradient descent.\n",
        "        @param X: training input data (N,)\n",
        "        @param y: training output data (N,)\n",
        "        @param lr: learning rate\n",
        "        @param batch_size\n",
        "        \"\"\"\n",
        "        for epoch in range(n_epochs):\n",
        "            # Prob-(a)\n",
        "            ## TODO (Start) ##\n",
        "            start = time.time()\n",
        "            # Creating mini barches\n",
        "            mini_batches = []\n",
        "            data = np.hstack((X.reshape((-1, 1)), y.reshape((-1, 1))))\n",
        "            np.random.shuffle(data)\n",
        "            num_batches = data.shape[0] // batch_size\n",
        "\n",
        "            for i in range(num_batches):\n",
        "                batch = data[i*batch_size:(i+1)*batch_size, :]\n",
        "                X_batch = batch[:, 0]\n",
        "                y_batch = batch[:, 1]\n",
        "                mini_batches.append((X_batch, y_batch))\n",
        "            # Take the rest of the data\n",
        "            if data.shape[0] % batch_size != 0:\n",
        "                batch = data[batch_size*num_batches+1:, :]\n",
        "                X_batch = batch[:, 0]\n",
        "                y_batch = batch[:, 1]\n",
        "                mini_batches.append((X_batch, y_batch))\n",
        "            for batch in mini_batches:\n",
        "                X_batch, y_batch = batch\n",
        "                gradients = self.backward(X_batch, y_batch)\n",
        "                \n",
        "                for param in self.params:\n",
        "                    self.params[param] -= lr * gradients[param]\n",
        "            current_loss = self.compute_loss(X, y)\n",
        "            end = time.time()\n",
        "            self.losses.append(current_loss)\n",
        "            self.times.append(end - start)\n",
        "            ## TODO (End) ##\n",
        "\n",
        "            print(\"========== Epoch {}/{} ==========\".format(epoch+1, n_epochs))\n",
        "            print(\"Loss > {:.2f}\".format(current_loss))\n",
        "            print(\"Params > coef: {:.2f} / bias: {:.2f}\".format(self.params['coef'], self.params['bias']))\n",
        "        self.times = list(accumulate(self.times))\n",
        "\n",
        "    def compute_loss(self, X, y):\n",
        "        \"\"\"\n",
        "        Computes the L2 loss of the model given X, y.\n",
        "        @param X: training input data (N,)\n",
        "        @param y: training output data (N,)\n",
        "        @return: loss\n",
        "        \"\"\"\n",
        "        a, b = self.params['coef'], self.params['bias']\n",
        "        loss = np.mean(np.power(y - (a*X + b), 2))\n",
        "        return loss\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBe7l0V7dIKR"
      },
      "source": [
        "## Test Full-Batch GD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cDT4WTYqOYS6"
      },
      "outputs": [],
      "source": [
        "model1 = SampleModel(a=0.5, b=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Egqtk40qtrQa"
      },
      "outputs": [],
      "source": [
        "X, y = model1.data_generation(true_a=20, true_b=5)\n",
        "\n",
        "## Use below code instead while doing Prob-(c)\n",
        "# X, y = model.data_generation_quad(true_a=20, true_b=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7Tmsvzqu26v",
        "outputId": "20593f95-24f0-4316-ed01-73b1ea6f1573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Starts...\n",
            "Full-Batch GD\n",
            "0.00510096549987793\n",
            "========== Epoch 1/30 ==========\n",
            "Loss > 260.39\n",
            "Params > coef: 4.40 / bias: 1.41\n",
            "0.0044896602630615234\n",
            "========== Epoch 2/30 ==========\n",
            "Loss > 168.05\n",
            "Params > coef: 7.52 / bias: 2.14\n",
            "0.0038323402404785156\n",
            "========== Epoch 3/30 ==========\n",
            "Loss > 108.96\n",
            "Params > coef: 10.02 / bias: 2.72\n",
            "0.003683328628540039\n",
            "========== Epoch 4/30 ==========\n",
            "Loss > 71.16\n",
            "Params > coef: 12.01 / bias: 3.18\n",
            "0.0036809444427490234\n",
            "========== Epoch 5/30 ==========\n",
            "Loss > 46.97\n",
            "Params > coef: 13.61 / bias: 3.55\n",
            "0.003178119659423828\n",
            "========== Epoch 6/30 ==========\n",
            "Loss > 31.49\n",
            "Params > coef: 14.89 / bias: 3.84\n",
            "0.0038499832153320312\n",
            "========== Epoch 7/30 ==========\n",
            "Loss > 21.59\n",
            "Params > coef: 15.91 / bias: 4.07\n",
            "0.002781391143798828\n",
            "========== Epoch 8/30 ==========\n",
            "Loss > 15.25\n",
            "Params > coef: 16.73 / bias: 4.26\n",
            "0.0028803348541259766\n",
            "========== Epoch 9/30 ==========\n",
            "Loss > 11.20\n",
            "Params > coef: 17.38 / bias: 4.41\n",
            "0.002850055694580078\n",
            "========== Epoch 10/30 ==========\n",
            "Loss > 8.60\n",
            "Params > coef: 17.91 / bias: 4.53\n",
            "0.003007173538208008\n",
            "========== Epoch 11/30 ==========\n",
            "Loss > 6.94\n",
            "Params > coef: 18.33 / bias: 4.62\n",
            "0.002112150192260742\n",
            "========== Epoch 12/30 ==========\n",
            "Loss > 5.88\n",
            "Params > coef: 18.66 / bias: 4.70\n",
            "0.0028297901153564453\n",
            "========== Epoch 13/30 ==========\n",
            "Loss > 5.20\n",
            "Params > coef: 18.93 / bias: 4.76\n",
            "0.0027086734771728516\n",
            "========== Epoch 14/30 ==========\n",
            "Loss > 4.77\n",
            "Params > coef: 19.14 / bias: 4.81\n",
            "0.002749204635620117\n",
            "========== Epoch 15/30 ==========\n",
            "Loss > 4.49\n",
            "Params > coef: 19.31 / bias: 4.85\n",
            "0.002837657928466797\n",
            "========== Epoch 16/30 ==========\n",
            "Loss > 4.31\n",
            "Params > coef: 19.45 / bias: 4.88\n",
            "0.0030243396759033203\n",
            "========== Epoch 17/30 ==========\n",
            "Loss > 4.20\n",
            "Params > coef: 19.56 / bias: 4.90\n",
            "0.0025763511657714844\n",
            "========== Epoch 18/30 ==========\n",
            "Loss > 4.12\n",
            "Params > coef: 19.65 / bias: 4.92\n",
            "0.0026290416717529297\n",
            "========== Epoch 19/30 ==========\n",
            "Loss > 4.08\n",
            "Params > coef: 19.72 / bias: 4.94\n",
            "0.0026710033416748047\n",
            "========== Epoch 20/30 ==========\n",
            "Loss > 4.05\n",
            "Params > coef: 19.78 / bias: 4.95\n",
            "0.002532958984375\n",
            "========== Epoch 21/30 ==========\n",
            "Loss > 4.03\n",
            "Params > coef: 19.82 / bias: 4.96\n",
            "0.002590179443359375\n",
            "========== Epoch 22/30 ==========\n",
            "Loss > 4.02\n",
            "Params > coef: 19.86 / bias: 4.97\n",
            "0.002580881118774414\n",
            "========== Epoch 23/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.89 / bias: 4.97\n",
            "0.0024945735931396484\n",
            "========== Epoch 24/30 ==========\n",
            "Loss > 4.00\n",
            "Params > coef: 19.91 / bias: 4.98\n",
            "0.0025119781494140625\n",
            "========== Epoch 25/30 ==========\n",
            "Loss > 4.00\n",
            "Params > coef: 19.93 / bias: 4.98\n",
            "0.0024971961975097656\n",
            "========== Epoch 26/30 ==========\n",
            "Loss > 4.00\n",
            "Params > coef: 19.94 / bias: 4.98\n",
            "0.0023682117462158203\n",
            "========== Epoch 27/30 ==========\n",
            "Loss > 4.00\n",
            "Params > coef: 19.95 / bias: 4.99\n",
            "0.002450227737426758\n",
            "========== Epoch 28/30 ==========\n",
            "Loss > 4.00\n",
            "Params > coef: 19.96 / bias: 4.99\n",
            "0.002451658248901367\n",
            "========== Epoch 29/30 ==========\n",
            "Loss > 4.00\n",
            "Params > coef: 19.97 / bias: 4.99\n",
            "0.002483367919921875\n",
            "========== Epoch 30/30 ==========\n",
            "Loss > 3.99\n",
            "Params > coef: 19.98 / bias: 4.99\n",
            "Time spent for model loss to be converged : 0.06347 sec\n",
            "Total epoch : 19\n",
            "Loss: 4.077084174467099\n",
            "Training Ends...\n",
            "\n",
            "Trained with 30 epochs, 0.1 learning rate\n",
            "Time Cost: 0.09683 sec\n"
          ]
        }
      ],
      "source": [
        "n_epochs, lr = 30, 1e-1\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "model1.run(X, y, train=True, n_epochs=n_epochs, lr=lr)\n",
        "loss = model1.compute_loss(X, y)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Time spent to converge\n",
        "for i in range(1, n_epochs):\n",
        "    loss_difference_rate = np.abs((model1.losses[i-1] - model1.losses[i]) / model1.losses[i-1] * 100)\n",
        "\n",
        "    if loss_difference_rate < 1:\n",
        "        print(\"Time spent for model loss to be converged : {:.5f} sec\".format(model1.times[i]))\n",
        "        print(\"Total epoch : {}\".format(i))\n",
        "        print(\"Loss: {}\".format(model1.losses[i-1]))\n",
        "        break\n",
        "\n",
        "print(\"Training Ends...\")\n",
        "print()\n",
        "print(\"Trained with {} epochs, {} learning rate\".format(n_epochs, lr))\n",
        "print(\"Time Cost: {:.5f} sec\".format(end_time-start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pOXQulGdNQE"
      },
      "source": [
        "## Test SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EhD_524TkKOC"
      },
      "outputs": [],
      "source": [
        "model2 = SampleModel(a=0.5, b=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gdNchXnokKZf"
      },
      "outputs": [],
      "source": [
        "X, y = model2.data_generation(true_a=20, true_b=5)\n",
        "\n",
        "## Use below code instead while doing Prob-(c)\n",
        "# X, y = model.data_generation_quad(true_a=20, true_b=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXC8rq4nkK2C",
        "outputId": "b0b24a14-3951-4550-9c21-b394a120396e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Starts...\n",
            "Mini-Batch SGD w Batch Size: 1\n",
            "========== Epoch 1/30 ==========\n",
            "Loss > 4.13\n",
            "Params > coef: 20.18 / bias: 5.31\n",
            "========== Epoch 2/30 ==========\n",
            "Loss > 4.36\n",
            "Params > coef: 19.67 / bias: 4.50\n",
            "========== Epoch 3/30 ==========\n",
            "Loss > 6.96\n",
            "Params > coef: 20.99 / bias: 6.41\n",
            "========== Epoch 4/30 ==========\n",
            "Loss > 5.36\n",
            "Params > coef: 20.97 / bias: 4.36\n",
            "========== Epoch 5/30 ==========\n",
            "Loss > 4.10\n",
            "Params > coef: 19.72 / bias: 4.85\n",
            "========== Epoch 6/30 ==========\n",
            "Loss > 5.44\n",
            "Params > coef: 21.15 / bias: 5.34\n",
            "========== Epoch 7/30 ==========\n",
            "Loss > 4.07\n",
            "Params > coef: 20.07 / bias: 4.74\n",
            "========== Epoch 8/30 ==========\n",
            "Loss > 6.26\n",
            "Params > coef: 21.42 / bias: 5.49\n",
            "========== Epoch 9/30 ==========\n",
            "Loss > 7.02\n",
            "Params > coef: 18.41 / bias: 5.70\n",
            "========== Epoch 10/30 ==========\n",
            "Loss > 5.44\n",
            "Params > coef: 20.47 / bias: 6.10\n",
            "========== Epoch 11/30 ==========\n",
            "Loss > 4.33\n",
            "Params > coef: 19.56 / bias: 4.63\n",
            "========== Epoch 12/30 ==========\n",
            "Loss > 4.21\n",
            "Params > coef: 20.43 / bias: 4.84\n",
            "========== Epoch 13/30 ==========\n",
            "Loss > 5.80\n",
            "Params > coef: 19.61 / bias: 6.28\n",
            "========== Epoch 14/30 ==========\n",
            "Loss > 4.16\n",
            "Params > coef: 19.96 / bias: 5.40\n",
            "========== Epoch 15/30 ==========\n",
            "Loss > 5.26\n",
            "Params > coef: 20.64 / bias: 4.08\n",
            "========== Epoch 16/30 ==========\n",
            "Loss > 7.14\n",
            "Params > coef: 18.26 / bias: 4.67\n",
            "========== Epoch 17/30 ==========\n",
            "Loss > 4.31\n",
            "Params > coef: 19.97 / bias: 5.55\n",
            "========== Epoch 18/30 ==========\n",
            "Loss > 6.36\n",
            "Params > coef: 18.97 / bias: 6.14\n",
            "========== Epoch 19/30 ==========\n",
            "Loss > 4.58\n",
            "Params > coef: 19.29 / bias: 4.72\n",
            "========== Epoch 20/30 ==========\n",
            "Loss > 5.15\n",
            "Params > coef: 21.05 / bias: 5.22\n",
            "========== Epoch 21/30 ==========\n",
            "Loss > 4.55\n",
            "Params > coef: 20.70 / bias: 5.24\n",
            "========== Epoch 22/30 ==========\n",
            "Loss > 5.98\n",
            "Params > coef: 18.81 / bias: 4.25\n",
            "========== Epoch 23/30 ==========\n",
            "Loss > 4.09\n",
            "Params > coef: 19.74 / bias: 5.15\n",
            "========== Epoch 24/30 ==========\n",
            "Loss > 6.03\n",
            "Params > coef: 21.37 / bias: 5.38\n",
            "========== Epoch 25/30 ==========\n",
            "Loss > 5.20\n",
            "Params > coef: 19.10 / bias: 5.63\n",
            "========== Epoch 26/30 ==========\n",
            "Loss > 4.82\n",
            "Params > coef: 19.51 / bias: 5.76\n",
            "========== Epoch 27/30 ==========\n",
            "Loss > 4.21\n",
            "Params > coef: 19.63 / bias: 4.72\n",
            "========== Epoch 28/30 ==========\n",
            "Loss > 4.06\n",
            "Params > coef: 20.25 / bias: 5.04\n",
            "========== Epoch 29/30 ==========\n",
            "Loss > 4.56\n",
            "Params > coef: 20.17 / bias: 4.27\n",
            "========== Epoch 30/30 ==========\n",
            "Loss > 4.51\n",
            "Params > coef: 20.12 / bias: 5.70\n",
            "Training Ends...\n",
            "\n",
            "Trained with 30 epochs, 0.1 learning rate\n",
            "Time Cost: 75.20367 sec\n"
          ]
        }
      ],
      "source": [
        "n_epochs, lr, batch_size = 30, 1e-1, 1\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "model2.run(X, y, train=True, n_epochs=n_epochs, lr=lr, batch_size=batch_size)\n",
        "loss = model1.compute_loss(X, y)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Time spent to converge\n",
        "for i in range(1, n_epochs):\n",
        "    loss_difference_rate = np.abs((model2.losses[i-1] - model2.losses[i]) / model2.losses[i-1] * 100)\n",
        "\n",
        "    if loss_difference_rate < 1:\n",
        "        print(\"Time spent for model loss to be converged : {:.5f} sec\".format(model2.times[i]))\n",
        "        print(\"Total epoch : {}\".format(i))\n",
        "        print(\"Loss: {}\".format(model2.losses[i-1]))\n",
        "        break\n",
        "\n",
        "print(\"Training Ends...\")\n",
        "print()\n",
        "print(\"Trained with {} epochs, {} learning rate\".format(n_epochs, lr))\n",
        "print(\"Time Cost: {:.5f} sec\".format(end_time-start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoaNYNFukGqx"
      },
      "source": [
        "## Test Mini-Batch SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YwbuV1afdOdj"
      },
      "outputs": [],
      "source": [
        "model3 = SampleModel(a=0.5, b=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HvAV_8R2dO10"
      },
      "outputs": [],
      "source": [
        "X, y = model3.data_generation(true_a=20, true_b=5)\n",
        "\n",
        "## Use below code instead while doing Prob-(c)\n",
        "# X, y = model.data_generation_quad(true_a=20, true_b=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4EvBKQcxd6Sg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Starts...\n",
            "Mini-Batch SGD w Batch Size: 1000\n",
            "========== Epoch 1/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.98 / bias: 5.00\n",
            "========== Epoch 2/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.96 / bias: 4.94\n",
            "========== Epoch 3/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.98 / bias: 4.99\n",
            "========== Epoch 4/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.99 / bias: 5.01\n",
            "========== Epoch 5/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 20.01 / bias: 5.01\n",
            "========== Epoch 6/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 20.00 / bias: 5.02\n",
            "========== Epoch 7/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.97 / bias: 5.00\n",
            "========== Epoch 8/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 20.02 / bias: 4.96\n",
            "========== Epoch 9/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 20.01 / bias: 5.00\n",
            "========== Epoch 10/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 20.00 / bias: 5.01\n",
            "========== Epoch 11/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.97 / bias: 4.98\n",
            "========== Epoch 12/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.97 / bias: 4.99\n",
            "========== Epoch 13/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 20.00 / bias: 4.99\n",
            "========== Epoch 14/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 20.03 / bias: 4.98\n",
            "========== Epoch 15/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.96 / bias: 4.99\n",
            "========== Epoch 16/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.99 / bias: 4.99\n",
            "========== Epoch 17/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.96 / bias: 5.00\n",
            "========== Epoch 18/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.97 / bias: 4.99\n",
            "========== Epoch 19/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.96 / bias: 4.99\n",
            "========== Epoch 20/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.99 / bias: 5.00\n",
            "========== Epoch 21/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.95 / bias: 4.98\n",
            "========== Epoch 22/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.96 / bias: 4.99\n",
            "========== Epoch 23/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.99 / bias: 4.97\n",
            "========== Epoch 24/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.99 / bias: 5.02\n",
            "========== Epoch 25/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 20.02 / bias: 5.00\n",
            "========== Epoch 26/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.99 / bias: 5.00\n",
            "========== Epoch 27/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.94 / bias: 4.99\n",
            "========== Epoch 28/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 20.00 / bias: 4.94\n",
            "========== Epoch 29/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.99 / bias: 4.97\n",
            "========== Epoch 30/30 ==========\n",
            "Loss > 4.01\n",
            "Params > coef: 19.97 / bias: 4.98\n",
            "Time spent for model loss to be converged : 0.21044 sec\n",
            "Total epoch : 1\n",
            "Loss: 4.01006261778625\n",
            "Training Ends...\n",
            "\n",
            "Trained with 30 epochs, 0.1 learning rate\n",
            "Time Cost: 3.00149 sec\n"
          ]
        }
      ],
      "source": [
        "n_epochs, lr, batch_size = 30, 1e-1, 1000\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "model3.run(X, y, train=True, n_epochs=n_epochs, lr=lr, batch_size=batch_size)\n",
        "loss = model1.compute_loss(X, y)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Time spent to converge\n",
        "for i in range(1, n_epochs):\n",
        "    loss_difference_rate = np.abs((model3.losses[i-1] - model3.losses[i]) / model3.losses[i-1] * 100)\n",
        "\n",
        "    if loss_difference_rate < 1:\n",
        "        print(\"Time spent for model loss to be converged : {:.5f} sec\".format(model3.times[i]))\n",
        "        print(\"Total epoch : {}\".format(i))\n",
        "        print(\"Loss: {}\".format(model3.losses[i-1]))\n",
        "        break\n",
        "\n",
        "print(\"Training Ends...\")\n",
        "print()\n",
        "print(\"Trained with {} epochs, {} learning rate\".format(n_epochs, lr))\n",
        "print(\"Time Cost: {:.5f} sec\".format(end_time-start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVpJJ9pf_vOm"
      },
      "source": [
        "## Result (b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performance test on Full-Batch GD, SGD, and Mini-Batch SGD is as follows. I conducted 5 times since SGD showed highly variable time spent.\n",
        "\n",
        "|                | Exp. 1 |        |          | Exp. 2 |        |          | Exp. 3 |         |          | Exp. 4 |         |          | Exp. 5 |         |          |\n",
        "|----------------|--------|--------|----------|--------|--------|----------|--------|---------|----------|--------|---------|----------|--------|---------|----------|\n",
        "|                | Epoch  | Loss   | Time (s) | Epoch  | Loss   | Time (s) | Epoch  | Loss    | Time (s) | Epoch  | Loss    | Time (s) | Epoch  | Loss    | Time (s) |\n",
        "| Full-Batch GD  | 19     | 4.083  | 0.08556  | 19     | 4.096  | 0.08066  | 19     | 4.08091 | 0.07246  | 19     | 4.07492 | 0.08083  | 19     | 4.09458 | 0.07751  |\n",
        "| SGD            | 7      | 4.1799 | 19.90088 | 24     | 4.2182 | 63.14866 | 5      | 4.19507 | 14.96709 | 16     | 4.60039 | 42.22355 | 11     | 4.13942 | 29.87885 |\n",
        "| Mini-Batch SGD | 1      | 3.9988 | 0.19164  | 1      | 3.9777 | 0.19326  | 1      | 3.97752 | 0.20482  | 1      | 3.98444 | 0.18896  | 1      | 4.00456 | 0.19578  |\n",
        "\n",
        "### Time\n",
        "Full-Batch GD spent the most less time for every experiment, followed by Mini-Batch SGD. Normal SGD spent the most time, about dozens ~ hundreds times compared to Full-Batch GD. It is because SGD considers every random error included in data, interrupts parameters from converging to accurate answer.\n",
        "\n",
        "### Loss\n",
        "Final losses are similar among GD methods. SGD showed the highest loss and Mini-Batch SGD did the lowest loss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "epochs = range(1, 31)\n",
        "plt.plot(epochs, model1.losses, 'r-')\n",
        "plt.plot(epochs, model2.losses, 'g-')\n",
        "plt.plot(epochs, model3.losses, 'b-')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Compare')\n",
        "\n",
        "plt.legend(['Full-Batch GD', 'SGD', 'Mini-Batch SGD'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZKnBGar_4Bf"
      },
      "source": [
        "## Result (c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "prob2_gd.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
